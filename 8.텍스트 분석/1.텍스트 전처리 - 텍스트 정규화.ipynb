{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac96b7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suhye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ac2c9",
   "metadata": {},
   "source": [
    "## 문장 토큰화\n",
    "마침표, 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리\n",
    "\n",
    "반환된 리스트 객체가 3개의 문장으로된 문자열을 가지고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37353ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to chuarch or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "#3개의 문장으로된 문서를 문장으로 분류\n",
    "text_sample = \"The Matrix is everywhere its all around us, here even in this room.\\\n",
    "                You can see it out your window or on your television. \\\n",
    "                You feel it when you go to work, or go to chuarch or pay your taxes.\"\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9cc90",
   "metadata": {},
   "source": [
    "## 단어 토큰화\n",
    "문장을 단어로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a16e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(type(words),len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f092038",
   "metadata": {},
   "source": [
    "sent_tokenize와 word_tokenize를 조합해 문서에 대해 모든 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9afe15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "#여러 문장으로된 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    #문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    #분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7215dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'chuarch', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "#여러문장에 대해 문장별 단어 토큰화\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adee08a",
   "metadata": {},
   "source": [
    "## 스톱워드 제거\n",
    "is, the, a. will 문맥적으로 의미가 없는 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fef27fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suhye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c3ff3",
   "metadata": {},
   "source": [
    "영어에 몇개의 stopwords가 있는지 확인\n",
    "\n",
    "그중 20개만 프린트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d9e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print(\"영어 stop words 개수 :\",len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967362d",
   "metadata": {},
   "source": [
    "words_token리스트에 대해 stopwords를 필터링으로 제거해 분석에 의미있는 단어만을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adfd57eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'chuarch', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "#3개의 문장별로 얻은 word_tokens_list에 대해 stopwords제거\n",
    "for sentence in word_tokens:\n",
    "    filltered_words = []\n",
    "    \n",
    "    #개별 문장별로 토큰화된 문장 list에 대해 stopwords 제거\n",
    "    for word in sentence:\n",
    "        \n",
    "        #소문자로 모두 변환\n",
    "        word = word.lower()\n",
    "        \n",
    "        #토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filltered_words.append(word)\n",
    "            \n",
    "    all_tokens.append(filltered_words)\n",
    "    \n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0be5e",
   "metadata": {},
   "source": [
    "## Stemming과 Lemmatization\n",
    "단어의 원형 찾기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75e6d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e0aeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suhye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ba4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
